{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ParticleEM/ParEM_anonymous/blob/main/bayesian_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5E8v2cZ6Zkf"
   },
   "source": [
    "**Description:** This notebook demonstrates the application of PGA, PQN, PMGA, and SOUL to the Bayesian neural network considered in Section 4.2 of 'Scalable particle-based alternatives to EM' and reproduces Figure 4 and Table 1 therein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XotQU1Ootpi6"
   },
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErKddtPzM3_H"
   },
   "source": [
    "First, we load the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QRkQ4RDsIAGf"
   },
   "outputs": [],
   "source": [
    "#@title Load modules.\n",
    "\n",
    "# Numpy and JAX for computations.\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Pyplot for plots.\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6VBGWiUNBLA"
   },
   "source": [
    "Next, we load and curate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mENE7XXBNLf0",
    "outputId": "7fdc1f91-db87-4169-f19f-0ca00854e4eb"
   },
   "outputs": [],
   "source": [
    "#@title Load, subsample, and normalize MNIST dataset.\n",
    "\n",
    "# Load dataset:\n",
    "from keras.datasets import mnist\n",
    "(images, labels), _ = mnist.load_data()\n",
    "images = np.array(images).astype(float)\n",
    "labels = np.array(labels).astype(int)\n",
    "\n",
    "# Keep only datapoints with labels 4 and 9:\n",
    "indices = (labels == 4) | (labels == 9)\n",
    "labels = labels[indices]\n",
    "images = images[indices, :, :]\n",
    "\n",
    "# Relabel as 4 as 0 and 9 as 1:\n",
    "for n in range(labels.size):\n",
    "    if labels[n] == 4:\n",
    "        labels[n] = 0\n",
    "    else:\n",
    "        labels[n] = 1\n",
    "\n",
    "# Sub-sample 1000 images:\n",
    "from sklearn.model_selection import train_test_split\n",
    "images, _, labels, _ = train_test_split(images, labels, train_size=1000,\n",
    "                                        random_state=0)\n",
    "\n",
    "# Normalize non-zero entries so that they have mean zero and unit standard \n",
    "# across the dataset:'''\n",
    "i = images.std(0) != 0\n",
    "images[:, i] = (images[:, i] - images[:, i].mean(0))/images[:, i].std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rf35vq3IC7T"
   },
   "source": [
    "We then implement the algorithms. They take in the following inputs:\n",
    "\n",
    "*   itrain : training set labels,\n",
    "*   itrain : training set images,\n",
    "*   itest : test set labels,\n",
    "*   itest : test set images,\n",
    "*   h : step-size,\n",
    "*   K : number of steps,\n",
    "*   N : number of particles,\n",
    "*   a : 1-dimensional vector with initial alpha guess,\n",
    "*   b : 1-dimensional vector with initial beta guess,\n",
    "*   w : Dw x N matrix storing the input layer weights of the initial particle cloud,\n",
    "*   v : Dv x N matrix storing the output layer weights of the initial particle cloud.\n",
    "\n",
    "They return the following outputs:\n",
    "\n",
    "*   a : K-dimensional vector of alpha estimates,\n",
    "*   b : K-dimensional vector of beta estimates,\n",
    "*   w : Dw x N matrix storing the input layer weights of the final particle cloud,\n",
    "*   v : Dv x N matrix storing the output layer weights of the final particle cloud,\n",
    "*   lppd : log pointwise predictive density (LPPD) as a function of k,\n",
    "*   error : test error as a function of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LtqjvuN98ZmU"
   },
   "outputs": [],
   "source": [
    "#@title Implement algorithms.\n",
    "\n",
    "# Algorithms.\n",
    "\n",
    "def pga(ltrain, itrain, ltest, itest, h, K, a, b, w, v):\n",
    "    # Extract dimensions of latent variables:\n",
    "    Dw = w[:, :, 0].size  # Dimension of w.\n",
    "    Dv = v[:, :, 0].size  # Dimension of v.\n",
    "\n",
    "    # Initialize arrays storing performance metrics as a function of k:\n",
    "    lppd = np.zeros(K)  # Log pointwise predictive density (LPPD).\n",
    "    error = np.zeros(K)  # Test error.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Evaluate metrics for current particle cloud:\n",
    "        lppd[k] = log_pointwise_predrictive_density(w, v, itest, ltest)\n",
    "        error[k] = test_error(w, v, itest, ltest)\n",
    "\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # Input layer weights.\n",
    "        vk = v  # Output layer weights.\n",
    "\n",
    "        # Update parameter estimates (note that we are using the heuristic \n",
    "        # discussed in Section 2.1 of the paper; i.e., dividing the \n",
    "        # alpha-gradient by Dw and the beta-gradient by Dv):\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(wk, a[k])/Dw)  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(vk, b[k])/Dv)  # Beta.\n",
    "\n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "    return a, b, w, v, lppd, error\n",
    "\n",
    "\n",
    "def pqn(ltrain, itrain, ltest, itest, h, K, a, b, w, v):\n",
    "    # Initialize arrays storing performance metrics as a function of k:\n",
    "    lppd = np.zeros(K)  # Log pointwise predictive density (LPPD).\n",
    "    error = np.zeros(K)  # Test error.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Evaluate metrics for current particle cloud:\n",
    "        lppd[k] = log_pointwise_predrictive_density(w, v, itest, ltest)\n",
    "        error[k] = test_error(w, v, itest, ltest)\n",
    "\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # Input layer weights.\n",
    "        vk = v  # Output layer weights\n",
    "\n",
    "        # Update parameter estimates:\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(wk, a[k])\n",
    "                                /ave_neg_hess_param(wk, a[k]))  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(vk, b[k])\n",
    "                                  /ave_neg_hess_param(vk, b[k]))  # Beta.\n",
    "        \n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "    return a, b, w, v, lppd, error\n",
    "\n",
    "\n",
    "def pmga(ltrain, itrain, ltest, itest, h, K, w, v):\n",
    "    # Initialize arrays storing performance metrics as a function of k:\n",
    "    lppd = np.zeros(K)  # Log pointwise predictive density (LPPD).\n",
    "    error = np.zeros(K)  # Test error.\n",
    "\n",
    "    # Compute initial parameter estimates:\n",
    "    a = np.array([optimal_param(w)])  # Alpha.\n",
    "    b = np.array([optimal_param(v)])  # Beta.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Evaluate metrics for current particle cloud:\n",
    "        lppd[k] = log_pointwise_predrictive_density(w, v, itest, ltest)\n",
    "        error[k] = test_error(w, v, itest, ltest)\n",
    "\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # Input layer weights.\n",
    "        vk = v  # Output layer weights.\n",
    "        \n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "        # Update parameter estimates:\n",
    "        a = np.append(a, optimal_param(w))  # Alpha.\n",
    "        b = np.append(b, optimal_param(v))  # Beta.\n",
    "\n",
    "    return a, b, w, v, lppd, error\n",
    "\n",
    "\n",
    "def soul(ltrain, itrain, ltest, itest, h, K, a, b, w, v):\n",
    "    # Extract dimensions of latent variables:\n",
    "    Dw = w[:, :, 0].size  # Dimension of w.\n",
    "    Dv = v[:, :, 0].size  # Dimension of v.\n",
    "\n",
    "    # Initialize arrays storing performance metrics as a function of k:\n",
    "    lppd = np.zeros(K)  # Log pointwise predictive density (LPPD).\n",
    "    error = np.zeros(K)  # Test error.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Evaluate metrics for current particle cloud:\n",
    "        lppd[k] = log_pointwise_predrictive_density(w, v, itest, ltest)\n",
    "        error[k] = test_error(w, v, itest, ltest) \n",
    "\n",
    "        # Initliaze new ULA chain at final state of previous ULA chain:\n",
    "        wkn = w[:, :, -1].reshape((w[:, 0, 0].size, w[0, :, 0].size, 1))\n",
    "        vkn = v[:, :, -1].reshape((v[:, 0, 0].size, v[0, :, 0].size, 1))\n",
    "\n",
    "        # Run ULA chain:\n",
    "        for n in range(w[0, 0, :].size):\n",
    "            # Take a step:\n",
    "            wkn = (wkn + h*wgrad(wkn, vkn, a[k], b[k], itrain, ltrain)\n",
    "                       + jnp.sqrt(2*h) * np.random.normal(0, 1, wkn.shape))\n",
    "            vkn = (vkn + h*vgrad(wkn, vkn, a[k], b[k], itrain, ltrain)\n",
    "                       + jnp.sqrt(2*h) * np.random.normal(0, 1, vkn.shape))\n",
    "            \n",
    "            # Store state:\n",
    "            w[:, :, n] = wkn.reshape(w[:, :, 0].shape)\n",
    "            v[:, :, n] = vkn.reshape(v[:, :, 0].shape)\n",
    "\n",
    "        # Update parameter estimates (note that we are using the heuristic \n",
    "        # discussed in Section 2.1 of the paper; i.e., dividing the \n",
    "        # alpha-gradient by Dw and the beta-gradient by Dv):'''\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(w, a[k])/Dw)  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(v, b[k])/Dv)  # Beta.\n",
    "\n",
    "    return a, b, w, v, lppd, error\n",
    "\n",
    "\n",
    "# Auxiliary functions.\n",
    "\n",
    "# Functions for the log density.\n",
    "\n",
    "def _log_nn(w, v, image):\n",
    "    # Log of the network's output when evaluated at image with weights w, v.\n",
    "    arg = jnp.dot(v, jnp.tanh(jnp.dot(w, image.reshape((28**2)))))\n",
    "    return jax.nn.log_softmax(arg)\n",
    "\n",
    "\n",
    "def _log_nn_vec(w, v, images):\n",
    "    # _log_nn vectorized over particles.\n",
    "    return jax.vmap(_log_nn, in_axes=(None, None, 0))(w, v, images)\n",
    "\n",
    "\n",
    "def _log_prior(x, lsig):\n",
    "    # Log of a Gaussian prior, with mean 0 and variance e^lsig, evaluated at x.\n",
    "    v = x.reshape((x.size))\n",
    "    sig = jnp.exp(lsig)\n",
    "    return -jnp.dot(v, v)/(2*sig**2) - x.size * (jnp.log(2*jnp.pi)/2 + lsig)\n",
    "\n",
    "\n",
    "def _log_likelihood(w, v, images, labels):\n",
    "    # Log-likelihood for set of images and labels, vectorized over particles.\n",
    "    return (_log_nn_vec(w, v, images)[jnp.arange(labels.size), labels]).sum()\n",
    "\n",
    "\n",
    "def _log_density(w, v, a, b, images, labels):\n",
    "    # Log of model density, vectorized over particles.\n",
    "    out = _log_prior(w, a) + _log_prior(v, b)\n",
    "    return out + _log_likelihood(w, v, images, labels)\n",
    "\n",
    "\n",
    "# Functions for the gradients of the log-density.'\n",
    "\n",
    "def _grad_param(x, lsig):\n",
    "    # Parameter gradient of one of the two log-priors.\n",
    "    v = x.reshape((x.size))\n",
    "    sig = jnp.exp(lsig)\n",
    "    return jnp.dot(v, v)/(sig**2) - x.size\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ave_grad_param(w, lsig):\n",
    "    \"\"\"Parameter gradient averaged over particle cloud.\"\"\"\n",
    "    grad = jax.vmap(_grad_param, in_axes=(2, None))(w, lsig)\n",
    "    return grad.mean()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def wgrad(w, v, a, b, images, labels):\n",
    "    \"\"\"w-gradient vectorized over particle cloud.\"\"\"\n",
    "    grad = jax.grad(_log_density, argnums=0)\n",
    "    gradv = jax.vmap(grad, in_axes=(2, 2, None, None, None, None), out_axes=2)\n",
    "    return gradv(w, v, a, b, images, labels)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def vgrad(w, v, a, b, images, labels):\n",
    "    \"\"\"v-gradients vectorized over particle cloud.\"\"\"\n",
    "    grad = jax.grad(_log_density, argnums=1)\n",
    "    gradv = jax.vmap(grad, in_axes=(2, 2, None, None, None, None), out_axes=2)\n",
    "    return gradv(w, v, a, b, images, labels)\n",
    "\n",
    "\n",
    "# Functions for the negative Hessian of the log prior used in the PQN update.\n",
    "\n",
    "def _neg_hess_param(x, lsig):\n",
    "    # Negative parameter Hessian of one of the two log-priors.'\n",
    "    v = x.reshape((x.size))\n",
    "    sig = jnp.exp(lsig)\n",
    "    return 2*jnp.dot(v, v)/(sig**2)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ave_neg_hess_param(w, lsig):\n",
    "    \"\"\"Negative parameter Hessian averaged over particles.\"\"\"\n",
    "    hess = jax.vmap(_neg_hess_param, in_axes=(2, None))(w, lsig)\n",
    "    return hess.mean()\n",
    "\n",
    "\n",
    "# Functions for the optimal parameters used in the PMGA update.\n",
    "\n",
    "def _normsq(x):\n",
    "    # Squared Frobenius norm of x.\n",
    "    v = x.reshape((x.size))\n",
    "    return jnp.dot(v, v)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def optimal_param(w):\n",
    "    \"\"\"Optimal parameter for weight cloud w.\"\"\"\n",
    "    mom2 = (jax.vmap(_normsq, in_axes=(2))(w)).mean()  # Second moment\n",
    "    return jnp.log(mom2/(w[:, :, 0].size))/2\n",
    "\n",
    "\n",
    "# Functions for the performance metrics.\n",
    "\n",
    "def _nn(w, v, image):\n",
    "    # Network's output when evaluated at image with weights w, v.\n",
    "    arg = jnp.dot(v, jnp.tanh(jnp.dot(w, image.reshape((28**2)))))\n",
    "    return jax.nn.softmax(arg)\n",
    "\n",
    "\n",
    "def _nn_vec(w, v, images):\n",
    "    # _nn vectorized over images.\n",
    "    return jax.vmap(_nn, in_axes=(None, None, 0))(w, v, images)\n",
    "\n",
    "\n",
    "def _nn_vec_vec(w, v, images):\n",
    "    # _nn_vec vectorized over particles.\n",
    "    return jax.vmap(_nn_vec, in_axes=(2, 2, None), out_axes=2)(w, v, images)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def log_pointwise_predrictive_density(w, v, images, labels):\n",
    "    \"\"\"Returns LPPD for set of (test) images and labels.\"\"\"\n",
    "    s = _nn_vec_vec(w, v, images).mean(2)\n",
    "    return jnp.log(s[jnp.arange(labels.size), labels]).mean()\n",
    "\n",
    "\n",
    "def _predict(w, v, images):\n",
    "    # Returns label maximizing the approximate posterior predictive \n",
    "    # distribution defined by the cloud (w,v), vectorized over images.\n",
    "    s = _nn_vec_vec(w, v, images).mean(2)\n",
    "    return jnp.argmax(s, axis=1)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def test_error(w, v, images, labels):\n",
    "    \"\"\"Returns fraction of misclassified images in test set.\"\"\"\n",
    "    return jnp.abs(labels - _predict(w, v, images)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKT-6l_6urur"
   },
   "source": [
    "We can now run the algorithms using an 80/20 training/test split of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qZD69GRu0G2"
   },
   "outputs": [],
   "source": [
    "# Split data into 80/20 training and testing sets:\n",
    "itrain, itest, ltrain, ltest = train_test_split(images, labels, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set approximation parameters:\n",
    "h = 1e-1 # Step-size. \n",
    "K = 500  # Number of steps.\n",
    "N = 100  # Number of particles.\n",
    "\n",
    "# Initialize parameter estimates:\n",
    "a0 = np.array([0])  # Alpha.\n",
    "b0 = np.array([0])  # Beta.\n",
    "\n",
    "# Initialize particle cloud by sampling prior:'\n",
    "w0 = np.exp(a0)*np.random.normal(0, 1, (40, 28**2, N))  # Input layer weights.\n",
    "v0 = np.exp(b0)*np.random.normal(0, 1, (2, 40, N))  # Output layer weights.\n",
    "\n",
    "# Run algorithms:\n",
    "a_pga, b_pga, w_pga, v_pga, lppd_pga, error_pga = pga(ltrain, itrain, ltest, \n",
    "                                                      itest, h, K, a0, b0, w0, \n",
    "                                                      v0)\n",
    "a_pqn, b_pqn, w_pqn, v_pqn, lppd_pqn, error_pqn = pqn(ltrain, itrain, ltest, \n",
    "                                                      itest, h, K, a0, b0, w0, \n",
    "                                                      v0)\n",
    "a_pmga, b_pmga, w_pmga, v_pmga, lppd_pmga, error_pmga = pmga(ltrain, itrain, \n",
    "                                                             ltest, itest, h, \n",
    "                                                             K, w0, v0)\n",
    "a_soul, b_soul, w_soul, v_soul, lppd_soul, error_soul = soul(ltrain, itrain, \n",
    "                                                             ltest, itest, h,\n",
    "                                                             K, a0, b0, w0, v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C0t3KlRxNON"
   },
   "source": [
    "As a sanity check, we plot the test error throughout training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "aEbGhK_-xl9R",
    "outputId": "76f980ad-00f0-4a91-c3cf-d2c8ec4317df"
   },
   "outputs": [],
   "source": [
    "plt.plot(error_pga, label='PGA') \n",
    "plt.plot(error_pqn, label='PQN')\n",
    "plt.plot(error_pmga, label='PMGA')\n",
    "plt.plot(error_soul, label='SOUL')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ryFlORx3DDW"
   },
   "source": [
    "As expected, the error decreases during training for all four algorithms. PGA, PQN, PMGA achieve noticeably smaller errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "T-B7NxVV3bYJ",
    "outputId": "ed3ef35e-508a-4f6a-cb13-080e793f8924"
   },
   "outputs": [],
   "source": [
    "plt.plot(error_pga, label='PGA') \n",
    "plt.plot(error_pqn, label='PQN')\n",
    "plt.plot(error_pmga, label='PMGA')\n",
    "plt.plot(error_soul, label='SOUL')\n",
    "plt.ylim([0, 0.1])\n",
    "plt.xlim([-K/100, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAMr6QD-ttmT"
   },
   "source": [
    "## Figure 4a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbsyJhPv34Sw"
   },
   "source": [
    "The parameter estimates seem to converge to various local optima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "0XNxrSsG4APV",
    "outputId": "e557489d-4d74-492a-9385-5f62ba7403bb"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(a_pga, label='PGA') \n",
    "plt.plot(a_pqn, label='PQN')\n",
    "plt.plot(a_pmga, label='PMGA')\n",
    "plt.plot(a_soul, label='SOUL')\n",
    "plt.title('Alpha estimates as a function of k')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.legend(loc='lower right', ncol=4)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(b_pga, label='PGA') \n",
    "plt.plot(b_pqn, label='PQN')\n",
    "plt.plot(b_pmga, label='PMGA')\n",
    "plt.plot(b_soul, label='SOUL')\n",
    "plt.title('Beta estimates as a function of k')\n",
    "plt.xlim([-K/100, K])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaMMf_Yu2TmH"
   },
   "source": [
    "## Figure 4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQLpNpkC4YGi"
   },
   "source": [
    "The posterior approximations produced by SOUL are more peaked than those produced by PGA, PQN, and PMGA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "B502rUyo4kaU",
    "outputId": "9cf5e035-a6e5-44c9-ded3-9d9df9772476"
   },
   "outputs": [],
   "source": [
    "# Choose a random weight for the input layer:\n",
    "oi = np.random.randint(0, w0[:, 0, 0].size)  # Output index.\n",
    "ii = np.random.randint(0, w0[0, :, 0].size)  # Input index.\n",
    "\n",
    "# Generate KDEs:\n",
    "from scipy import stats\n",
    "\n",
    "kde_min = np.min([w_pga[oi, ii, :], w_pqn[oi, ii, :], \n",
    "              w_pmga[oi, ii, :], w_soul[oi, ii, :]])\n",
    "kde_max = np.max([w_pga[oi, ii, :], w_pqn[oi, ii, :],\n",
    "              w_pmga[oi, ii, :], w_soul[oi, ii, :]])\n",
    "xaxis = np.linspace(kde_min, kde_max, num=100)\n",
    "\n",
    "kde_pga = stats.gaussian_kde(w_pga[oi, ii, :])(xaxis)\n",
    "kde_pqn = stats.gaussian_kde(w_pqn[oi, ii, :])(xaxis)\n",
    "kde_pmga = stats.gaussian_kde(w_pmga[oi, ii, :])(xaxis)\n",
    "kde_soul = stats.gaussian_kde(w_soul[oi, ii, :])(xaxis)\n",
    "\n",
    "# Plot KDEs:\n",
    "plt.plot(xaxis, kde_pga, label='PGA')\n",
    "plt.plot(xaxis, kde_pqn, label='PQN')\n",
    "plt.plot(xaxis, kde_pmga, label='PMGA')\n",
    "plt.plot(xaxis, kde_soul, label='SOUL')\n",
    "plt.ylim([0, 1.02*kde_soul.max()])\n",
    "plt.xlim([kde_min, kde_max])\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJr5lfAX2U5U"
   },
   "source": [
    "## Figure 4c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXLv7osg9pSm"
   },
   "source": [
    "This is likely the reason why PGA, PQN, and PMGA achieve lower errors than SOUL. We observe the same sort of behaviour if we plot log pointwise predictive densities rather than test errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "0asBF5qxAcmH",
    "outputId": "d6828e3b-683b-43c2-ee68-b3b6084ff593"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(lppd_pga, label='PGA') \n",
    "plt.plot(lppd_pqn, label='PQN')\n",
    "plt.plot(lppd_pmga, label='PMGA')\n",
    "plt.plot(lppd_soul, label='SOUL')\n",
    "plt.title('LPPD as a function of k')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.legend(loc='lower right', ncol=4)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(lppd_pga, label='PGA') \n",
    "plt.plot(lppd_pqn, label='PQN')\n",
    "plt.plot(lppd_pmga, label='PMGA')\n",
    "plt.plot(lppd_soul, label='SOUL')\n",
    "plt.title('LPPD as a function of k (zoomed-in)')\n",
    "plt.ylim([-0.16, -0.08])\n",
    "plt.xlim([-K/100, K])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T62bmxVZ2V6q"
   },
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_STypNJCd0P"
   },
   "source": [
    "We benchmark the predictive performances and computation times of the algorithms by running them several times. To this end, we remove the per-time-step performance metric calculations from the algorithms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lc_eAPooDKdO"
   },
   "outputs": [],
   "source": [
    "#@title Algorithms without LPPD and test error computations.\n",
    "\n",
    "def pga(ltrain, itrain, h, K, a, b, w, v):\n",
    "    # Extract dimensions of latent variables:\n",
    "    Dw = w[:, :, 0].size  # Dimension of w.\n",
    "    Dv = v[:, :, 0].size  # Dimension of v.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # w components.\n",
    "        vk = v  # v components.\n",
    "\n",
    "        # Update parameter estimates (note that we are using the heuristic \n",
    "        # discussed in Section 2.1 of the paper; i.e., dividing the alpha-gradient\n",
    "        # by Dw and the beta-gradient by Dv):'''\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(wk, a[k])/Dw)  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(vk, b[k])/Dv)  # Beta.\n",
    "\n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "    return a, b, w, v\n",
    "\n",
    "\n",
    "def pqn(ltrain, itrain, h, K, a, b, w, v):\n",
    "\n",
    "    for k in range(K):\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # w components.\n",
    "        vk = v  # v components.\n",
    "\n",
    "        # Update parameter estimates:\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(wk, a[k])\n",
    "                                  /ave_neg_hess_param(wk, a[k]))  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(vk, b[k])\n",
    "                                  /ave_neg_hess_param(vk, b[k]))  # Beta.\n",
    "        \n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "    return a, b, w, v\n",
    "\n",
    "\n",
    "def pmga(ltrain, itrain, h, K, w, v):\n",
    "    # Compute initial parameter estimates:\n",
    "    a = np.array([optimal_param(w)])  # Alpha estimates.\n",
    "    b = np.array([optimal_param(v)])  # Beta estimates.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Temporarily store current particle cloud:\n",
    "        wk = w  # w components.\n",
    "        vk = v  # v components.\n",
    "        \n",
    "        # Update particle cloud:\n",
    "        w = (w + h*wgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, w.shape)) \n",
    "        v = (v + h*vgrad(wk, vk, a[k], b[k], itrain, ltrain)\n",
    "               + jnp.sqrt(2*h) * np.random.normal(0, 1, v.shape))\n",
    "\n",
    "        # Update parameter estimates:\n",
    "        a = np.append(a, optimal_param(w))  # Alpha.\n",
    "        b = np.append(b, optimal_param(v))  # Beta.\n",
    "\n",
    "    return a, b, w, v\n",
    "\n",
    "\n",
    "def soul(ltrain, itrain, h, K, a, b, w, v):\n",
    "    # Extract dimensions of latent variables:\n",
    "    Dw = w[:, :, 0].size  # Dimension of w.\n",
    "    Dv = v[:, :, 0].size  # Dimension of v.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Initliaze new ULA chain at final state of previous ULA chain:\n",
    "        wkn = w[:, :, -1].reshape((w[:, 0, 0].size, w[0, :, 0].size, 1))\n",
    "        vkn = v[:, :, -1].reshape((v[:, 0, 0].size, v[0, :, 0].size, 1))\n",
    "\n",
    "        # Run ULA chain:\n",
    "        for n in range(w[0, 0, :].size):\n",
    "            # Take a step:\n",
    "            wkn = (wkn + h*wgrad(wkn, vkn, a[k], b[k], itrain, ltrain)\n",
    "                       + jnp.sqrt(2*h) * np.random.normal(0, 1, wkn.shape))\n",
    "            vkn = (vkn + h*vgrad(wkn, vkn, a[k], b[k], itrain, ltrain)\n",
    "                       + jnp.sqrt(2*h) * np.random.normal(0, 1, vkn.shape))\n",
    "            \n",
    "            # Store state:\n",
    "            w[:, :, n] = wkn.reshape(w[:, :, 0].shape)\n",
    "            v[:, :, n] = vkn.reshape(v[:, :, 0].shape)\n",
    "\n",
    "        # Update parameter estimates (note that we are using the heuristic \n",
    "        # discussed in Section 2.1 of the paper; i.e., dividing the \n",
    "        # alpha-gradient by Dw and the beta-gradient by Dv):\n",
    "        a = np.append(a, a[k] + h*ave_grad_param(w, a[k])/Dw)  # Alpha.\n",
    "        b = np.append(b, b[k] + h*ave_grad_param(v, b[k])/Dv)  # Beta.\n",
    "\n",
    "    return a, b, w, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHf8uU4FUMhr"
   },
   "source": [
    "## Predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evxgxACjMhfc"
   },
   "source": [
    "We start with the performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nO6q_z88Mlyh",
    "outputId": "35e99d26-0f3f-46b2-caa0-e5c4f59d5be5"
   },
   "outputs": [],
   "source": [
    "M = 10 # Number of runs.\n",
    "N = 100 # Number of particles.\n",
    "\n",
    "# Initialize arrays storing the LPPDs and test errors:\n",
    "lppd_pga = np.zeros((M))\n",
    "lppd_pqn = np.zeros((M))\n",
    "lppd_pmga = np.zeros((M))\n",
    "lppd_soul = np.zeros((M))\n",
    "\n",
    "error_pga = np.zeros((M))\n",
    "error_pqn = np.zeros((M))\n",
    "error_pmga = np.zeros((M))\n",
    "error_soul = np.zeros((M))\n",
    "\n",
    "for m in range(M):\n",
    "    print('Run ' + str(m) + ' out of ' + str(M) + '.')\n",
    "    \n",
    "    # Randomize the initialization of the particle cloud:\n",
    "    w0 = np.exp(a0)*np.random.normal(0, 1, (40, 28**2, N))  # w-components.\n",
    "    v0 = np.exp(b0)*np.random.normal(0, 1, (2, 40, N))  # v-components.\n",
    "    \n",
    "    # Randomize the 80/20 testing/training split of the data:\n",
    "    itrain, itest, ltrain, ltest = train_test_split(images, labels, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=m)\n",
    "\n",
    "    # Run the algorithms:\n",
    "    _, _, w_pga, v_pga = pga(ltrain, itrain, h, K, a0, b0, w0, v0)\n",
    "    _, _, w_pqn, v_pqn = pqn(ltrain, itrain, h, K, a0, b0, w0, v0)\n",
    "    _, _, w_pmga, v_pmga = pmga(ltrain, itrain, h, K, w0, v0)\n",
    "    _, _, w_soul, v_soul = soul(ltrain, itrain, h, K, a0, b0, w0, v0)\n",
    "\n",
    "    # Compute LPPDs:\n",
    "    lppd_pga[m] = log_pointwise_predrictive_density(w_pga, v_pga, itest, ltest)\n",
    "    lppd_pqn[m] = log_pointwise_predrictive_density(w_pqn, v_pqn, itest, ltest)\n",
    "    lppd_pmga[m] = log_pointwise_predrictive_density(w_pmga, v_pmga, itest, ltest)\n",
    "    lppd_soul[m] = log_pointwise_predrictive_density(w_soul, v_soul, itest, ltest)\n",
    "\n",
    "    # Compute test errors:\n",
    "    error_pga[m] = test_error(w_pga, v_pga, itest, ltest)\n",
    "    error_pqn[m] = test_error(w_pqn, v_pqn, itest, ltest)\n",
    "    error_pmga[m] = test_error(w_pmga, v_pmga, itest, ltest)\n",
    "    error_soul[m] = test_error(w_soul, v_soul, itest, ltest)\n",
    "\n",
    "# Print statistics (mean +- standard deviation):\n",
    "print('LPPDs:')\n",
    "print('PGA: '+str(lppd_pga.mean())+' \\u00B1 '+str(lppd_pga.std())+'.')\n",
    "print('PQN: '+str(lppd_pqn.mean())+' \\u00B1 '+str(lppd_pqn.std())+'.')\n",
    "print('PMGA: '+str(lppd_pmga.mean())+' \\u00B1 '+str(lppd_pmga.std())+'.')\n",
    "print('SOUL: '+str(lppd_soul.mean())+' \\u00B1 '+str(lppd_soul.std())+'.')\n",
    "\n",
    "print('Test errors:')\n",
    "print('PGA: '+str(error_pga.mean())+' \\u00B1 '+str(error_pga.std())+'.')\n",
    "print('PQN: '+str(error_pqn.mean())+' \\u00B1 '+str(error_pqn.std())+'.')\n",
    "print('PMGA: '+str(error_pmga.mean())+' \\u00B1 '+str(error_pmga.std())+'.')\n",
    "print('SOUL: '+str(error_soul.mean())+' \\u00B1 '+str(error_soul.std())+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb2K-quMUSHt"
   },
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9qax-8sEifB"
   },
   "source": [
    "Lastly, we obtain runtime statistics using IPython's timeit magic (note that Colab can be set to run on a CPU, GPU, or TPU by selecting Runtime --> Change runtime type --> Hardware accelerator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9OEhxkTYZhD"
   },
   "outputs": [],
   "source": [
    "from timeit import repeat\n",
    "\n",
    "# Time the algorithms:\n",
    "t_pga = np.array(repeat(stmt='pga(ltrain, itrain, h, K, a0, b0, w0, v0)',\n",
    "                       number=1, repeat=M, globals=globals()))\n",
    "t_pqn = np.array(repeat(stmt='pqn(ltrain, itrain, h, K, a0, b0, w0, v0)',\n",
    "                       number=1, repeat=M, globals=globals()))\n",
    "t_pmga = np.array(repeat(stmt='pmga(ltrain, itrain, h, K, w0, v0)',\n",
    "                        number=1, repeat=M, globals=globals()))\n",
    "t_soul = np.array(repeat(stmt='soul(ltrain, itrain, h, K, a0, b0, w0, v0)',\n",
    "                        number=1, repeat=M, globals=globals()))\n",
    "\n",
    "# Print statistics (mean +- standard deviation):\n",
    "print('Runtimes:')\n",
    "print('PGA: '+str(t_pga.mean())+' \\u00B1 '+str(t_pga.std())+'.')\n",
    "print('PQN: '+str(t_pqn.mean())+' \\u00B1 '+str(t_pqn.std())+'.')\n",
    "print('PMGA: '+str(t_pmga.mean())+' \\u00B1 '+str(t_pmga.std())+'.')\n",
    "print('SOUL: '+str(t_soul.mean())+' \\u00B1 '+str(t_soul.std())+'.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO88Au/qnwOChdiyowWtwZY",
   "collapsed_sections": [
    "PAMr6QD-ttmT",
    "KaMMf_Yu2TmH",
    "yJr5lfAX2U5U",
    "T62bmxVZ2V6q"
   ],
   "include_colab_link": true,
   "name": "Bayesian Neural Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
