{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ParticleEM/ParEM_anonymous/blob/main/toy_hierarchical_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5E8v2cZ6Zkf"
   },
   "source": [
    "**Description:** This notebook demonstrates the application of PGA, PQN, PMGA, and EM to the toy hierarchical model in Example 1 of 'Scalable particle-based alternatives to EM' and reproduces Figures 1 and 2 therein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XotQU1Ootpi6"
   },
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rf35vq3IC7T"
   },
   "source": [
    "To start, we load the modules we need and implement the algorithms. The algorithms take the following inputs:\n",
    "\n",
    "*   y : D-dimensional vector of observations,\n",
    "*   h : step-size,\n",
    "*   K : number of steps,\n",
    "*   N : number of particles,\n",
    "*   th : 1-dimensional vector containing the initial parameter guess,\n",
    "*   X : D x N matrix containing the initial particle cloud;\n",
    "\n",
    "and return a single output:\n",
    "\n",
    "*   th : K-dimensional vector of parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "QRkQ4RDsIAGf"
   },
   "outputs": [],
   "source": [
    "#@title Load modules.\n",
    "import numpy as np  # Numpy for computations.\n",
    "import matplotlib.pyplot as plt  # Pyplot for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LtqjvuN98ZmU"
   },
   "outputs": [],
   "source": [
    "#@title Implement algorithms.\n",
    "\n",
    "# Algorithms.\n",
    "\n",
    "def pga(y, h, K, N, th, X):\n",
    "    \"\"\"Particle Gradient Ascent Algorithm. Returns parameter estimates.\"\"\"\n",
    "    D = y.size  # Extract dimension of latent variables.\n",
    "    for k in range(K):\n",
    "        # Update parameter estimate:\n",
    "        th = np.append(th, th[k] + h*ave_grad_th(th[k], X))\n",
    "        # Update particle cloud:\n",
    "        X = (X + h*grad_x(y, th[k], X)\n",
    "               + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "    return th\n",
    "\n",
    "\n",
    "def pqn(y, h, K, N, th, X):\n",
    "    \"\"\"Particle Quasi-Newton Algorithm. Returns parameter estimates.\"\"\"\n",
    "    D = y.size  # Extract dimension of latent variables.\n",
    "    for k in range(K):\n",
    "        # Update parameter estimate:\n",
    "        th = np.append(th, th[k] \n",
    "                            + h*(ave_neg_hess_th(D)**-1)*ave_grad_th(th[k], X))\n",
    "        # Update particle cloud:\n",
    "        X = (X + h*grad_x(y, th[k], X)\n",
    "               + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "    return th\n",
    "\n",
    "\n",
    "def pmga(y, h, K, N, X):\n",
    "    \"\"\"Particle Marginal Gradient Ascent Algorithm. \n",
    "    Returns parameter estimates.\n",
    "    \"\"\"\n",
    "    D = y.size  # Extract dimension of latent variables.\n",
    "    th = np.array([theta_opt(X)])  # Compute initial parameter estimate.\n",
    "    for k in range(K):\n",
    "        # Update particle cloud:\n",
    "        X = (X + h*grad_x(y, th[k], X)\n",
    "               + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        th = np.append(th, theta_opt(X))  # Update parameter estimate.\n",
    "    return th\n",
    "\n",
    "\n",
    "def em(y, K, th):\n",
    "    \"\"\"Expectation Maximization Algorithm. Returns parameter estimates.\"\"\"\n",
    "    for k in range(K):\n",
    "        th = np.append(th, th[k]/2 + y.mean()/2)  # Update parameter estimate.\n",
    "    return th\n",
    "\n",
    "\n",
    "# Auxiliary functions.\n",
    "\n",
    "def theta_opt(X):\n",
    "    return X.mean()  # Return optimal parameter for particle cloud X.\n",
    "\n",
    "\n",
    "def ave_grad_th(th, X):\n",
    "    \"\"\"Returns theta-gradient of log density averaged over particles.\"\"\"\n",
    "    return X[:, 0].size*(theta_opt(X) - th)\n",
    "\n",
    "\n",
    "def ave_neg_hess_th(D):\n",
    "    \"\"\"Returns negative-theta-Hessian of log density averaged over particles.\n",
    "    \"\"\"\n",
    "    return D\n",
    "\n",
    "\n",
    "def grad_x(y, th, X):\n",
    "    \"\"\"Returns x-gradient of log density vectorized over particles.\"\"\"\n",
    "    return (y + th - 2*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAOkznJuHiMG"
   },
   "source": [
    "Next, we choose the model parameters and we generate synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfZWlqFdHqW5"
   },
   "outputs": [],
   "source": [
    "D = 100  # Dimensionality of latent variables.\n",
    "thdata = 1  # Parameter value used to generate the data.\n",
    "\n",
    "# Generate the data:\n",
    "y = np.random.normal(0, 1, (D, 1)) + np.random.normal(thdata, 1, (D, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAMr6QD-ttmT"
   },
   "source": [
    "## Figure 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPnKONzlHagU"
   },
   "source": [
    "We examine the impact that different step-sizes have on PGA's stability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "kkmn65RaY5uk",
    "outputId": "ed8ff3bf-767e-4912-cb2a-27c41bf0f80f"
   },
   "outputs": [],
   "source": [
    "#Set approximation parameters:\n",
    "K = 300  # Number of steps.\n",
    "N = 10  # Number of particles.\n",
    "\n",
    "th0 = np.array([0])  # Initial parameter guess.\n",
    "X0 = np.zeros((D, N))  # Initial particle cloud.\n",
    "\n",
    "# Run PGA using three different step-sizes:\n",
    "th_large = pga(y, 2.05/(2+D), K, N, th0, X0)  # Large step-size\n",
    "th_optimal = pga(y, 2/(2+D), K, N, th0, X0)  # Optimal step-size \n",
    "th_small = pga(y, 0.75/(2+D), K, N, th0, X0)  # Small step-size\n",
    "\n",
    "# Plot parameter estimates as a function of step number k:\n",
    "plt.plot(th_large, label='Large step-size')\n",
    "plt.plot(th_optimal, label='Optimal step-size')\n",
    "plt.plot(th_small, label='Small step-size')\n",
    "plt.plot(y.mean()*np.ones(K), label='Optimal theta')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0, 1.2*y.mean()])\n",
    "plt.xlim([0, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcsX2yOfakVA"
   },
   "source": [
    "## Figure 1b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTacJBGDtxlB"
   },
   "source": [
    "We compare the performance of PGA, PQN, PMGA, and EM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "81Z7zf_THPPl",
    "outputId": "93a0b118-7461-43ab-be54-e12b3594d1ee"
   },
   "outputs": [],
   "source": [
    "# Run the algorithms using the optimal step-sizes:\n",
    "th_pga = pga(y, 2/(2+D), K, N, th0, X0)\n",
    "th_pqn = pqn(y, 2/3, K, N, th0, X0)\n",
    "th_pmga = pmga(y, 1, K, N, X0)\n",
    "th_em = em(y, K, th0)\n",
    "\n",
    "# Plot parameter estimates as a function of step number k:\n",
    "plt.plot(th_pga, label='PGA')\n",
    "plt.plot(th_pqn, label='PQN')\n",
    "plt.plot(th_pmga, label='PMGA')\n",
    "plt.plot(th_em, label='EM')\n",
    "plt.plot(y.mean()*np.ones(K), label='Optimal theta')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-K/100, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBmRGCcXfNc8"
   },
   "source": [
    "To extract converging estimates from PGA, PQN, and PMGA, we average over time (starting once the estimates reach stationarity). To this end, we use the following function that cumulatively averages all entries of a vector x past a threshold n: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Rcc1TQ2acBEK"
   },
   "outputs": [],
   "source": [
    "#@title Cumulative mean.\n",
    "\n",
    "def cmean(x, n):\n",
    "    \"\"\"Returns [x[0], ..., x[n-1], z[n], ..., z[K-1]], where N denotes x's size\n",
    "    and z[k] denotes the average of [x[n], ..., x[k]].\n",
    "    \"\"\"\n",
    "    if n == 0:\n",
    "      return np.cumsum(x[n:-1])/np.arange(1, x[n:-1].size + 1)\n",
    "    else:\n",
    "      return np.append(x[0:n-1],\n",
    "                        np.cumsum(x[n:-1])/np.arange(1, x[n:-1].size + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWhgGvakfbam"
   },
   "source": [
    "We then obtain the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "bE_qEJHEfeZi",
    "outputId": "67795883-14fd-4a2e-a345-b3b23294edd1"
   },
   "outputs": [],
   "source": [
    "plt.plot(cmean(th_pga, 150), label='PGA')\n",
    "plt.plot(cmean(th_pqn, 15), label='PQN')\n",
    "plt.plot(cmean(th_pmga, 5), label='PMGA')\n",
    "plt.plot(th_em, label='EM')\n",
    "plt.plot(y.mean()*np.ones(K), label='Optimal theta')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-K/100, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3voD52-gFjT"
   },
   "source": [
    "Lastly, we zoom in to the first 30 time-steps to differentiate between the PQN, PMGA, and EM estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "GNwvMNkZg7YL",
    "outputId": "bce5a6a0-cf9e-4b5e-8f81-f1947f6a5766"
   },
   "outputs": [],
   "source": [
    "plt.plot(cmean(th_pga, 150), label='PGA')\n",
    "plt.plot(cmean(th_pqn, 15), label='PQN')\n",
    "plt.plot(cmean(th_pmga, 5), label='PMGA')\n",
    "plt.plot(th_em, label='EM')\n",
    "plt.plot(y.mean()*np.ones(K), label='Optimal theta')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim([-0.3, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec6W0Oo1hM16"
   },
   "source": [
    "# Figure 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tODqEyMXt2m_"
   },
   "source": [
    "We investigate the asymptotic bias present in the variance of the posterior approximations produced by PMGA (time-averaged without burn-in). We focus on the uni-dimensional case (D=1) for which the bias is most pronounced. This requires generating new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzYm42omqY60"
   },
   "outputs": [],
   "source": [
    "D = 1  # Dimensionality of latent variables.\n",
    "\n",
    "# Generate the data:\n",
    "y = np.random.normal(0, 1, (D, 1)) + np.random.normal(thdata, 1, (D, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fk2gL-YfqQoT"
   },
   "source": [
    "Next, we tweak the PMGA code so that it returns (time-averaged without burn-in) variance estimates rather parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "9D0bDFXZqmL_"
   },
   "outputs": [],
   "source": [
    "#@title Tweaked PMGA\n",
    "\n",
    "def pmga_v(y, h, K, N, X):\n",
    "    \"\"\"Particle Marginal Gradient Ascent Algorithm. \n",
    "    Returns posterior variance estimates.\n",
    "    \"\"\"\n",
    "    D = y.size  # Extract dimension of latent variables.\n",
    "    th = np.array([theta_opt(X)])  # Compute initial parameter estimate.\n",
    "\n",
    "    # Initialize the vectors that will contain the first two moments of the\n",
    "    # current particle cloud (required to compute the variance estimates):\n",
    "    mu1 = np.reshape(X.mean(1), (D, 1))  # First moment.\n",
    "    mu2 = np.reshape((X ** 2).mean(1), (D, 1)) # Second moment.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Update particle cloud:\n",
    "        X = (X + h*grad_x(y, th[k], X)\n",
    "               + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        th = np.append(th, theta_opt(X)) # Update parameter estimate.\n",
    "        \n",
    "        # Store moments:\n",
    "        mu1 = np.append(mu1, np.reshape(X.mean(1), (D, 1)), axis=1)\n",
    "        mu2 = np.append(mu2, np.reshape((X ** 2).mean(1), (D, 1)), axis=1)\n",
    "\n",
    "    # Compute time-averaged variance estimates as a function of k:\n",
    "    var = (np.cumsum(mu2)/np.arange(1, K + 2) \n",
    "           - (np.cumsum(mu1)/np.arange(1, K + 2)) ** 2)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXeTOIZy-tZA"
   },
   "source": [
    "## Figure 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9zb7kte-v1W"
   },
   "source": [
    "In our first plot, we examine the dependence of the bias on the particle number. To this end, we fix a small step-size and large step number. In short, the larger the particle number, the smaller the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "RZwY9_Q5_aam",
    "outputId": "a8ec5493-d46b-400d-cfb3-cf224dd4dfc8"
   },
   "outputs": [],
   "source": [
    "K = 20000  # Number of steps.\n",
    "h = 0.015  # Step-size.\n",
    "\n",
    "# Compute variance estimates:\n",
    "var1 = pmga_v(y, h, K, 1, np.zeros((D, 1)))  # Using 1 particle.\n",
    "var2 = pmga_v(y, h, K, 2, np.zeros((D, 2)))  # Using 2 particles.\n",
    "var4 = pmga_v(y, h, K, 4, np.zeros((D, 4)))  # Using 4 particles.\n",
    "var50 = pmga_v(y, h, K, 50, np.zeros((D, 50)))  # Using 50 particle.\n",
    "var100 = pmga_v(y, h, K, 100, np.zeros((D, 100)))  # Using 100 particle.\n",
    "\n",
    "# Plot estimates:\n",
    "plt.plot(var1, label='N = 1')\n",
    "plt.plot(var2, label='N = 2')\n",
    "plt.plot(var4, label='N = 4')\n",
    "plt.plot(var50, label='N = 50')\n",
    "plt.plot(var100, label='N = 100')\n",
    "plt.plot(0.5*np.ones(K), label='Optimal variance')\n",
    "plt.legend(loc='lower right', ncol=2)\n",
    "plt.xlim([-K/100, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElXW-4d9B9nc"
   },
   "source": [
    "## Figure 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5OKTRyxCDBG"
   },
   "source": [
    "Next, we examine how the bias depends on the step-size. To do so, we fix large particle and step numbers. This time, the smaller the step-size, the smaller the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "EjCUi1NGCd0C",
    "outputId": "f4b1acd0-6af1-46a7-bdd2-8e58fb7e0be0"
   },
   "outputs": [],
   "source": [
    "N = 100  # Number of particles. \n",
    "\n",
    "X0 = np.zeros((D, N))  # Initial particle cloud (with amended dimension).\n",
    "\n",
    "# Compute variance estimates:\n",
    "var05 = pmga_v(y, 1/2, K, N, X0)  # Using a step-size of 1/2.\n",
    "var025 = pmga_v(y, 1/4, K, N, X0)  # Using a step-size of 1/4.\n",
    "var0125 = pmga_v(y, 1/8, K, N, X0)  # Using a step-size of 1/8.\n",
    "var003 = pmga_v(y, 0.03, K, N, X0)  # Using a step-size of 0.03.\n",
    "var0015 = pmga_v(y, 0.015, K, N, X0)  # Using a step-size of 0.015.\n",
    "\n",
    "# Plot estimates:\n",
    "plt.plot(var05, label='h = 1/2')\n",
    "plt.plot(var025, label='h = 1/4')\n",
    "plt.plot(var0125, label='h = 1/8')\n",
    "plt.plot(var003, label='h = 0.03')\n",
    "plt.plot(var0015, label='h = 0.015')\n",
    "plt.plot(0.5*np.ones(K), label='Optimal variance')\n",
    "plt.legend(loc='lower right', ncol=2)\n",
    "plt.xlim([-K/100, K])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OVE77mEB_2I"
   },
   "source": [
    "## Figure 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD-ILtbqDoxi"
   },
   "source": [
    "We can remove the bias stemming from the discretization of the time-axis by adding a population-wide accept-reject step. In other words, by running the following Metropolized version of PMGA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Lj6eFcL9EKjd"
   },
   "outputs": [],
   "source": [
    "#@title Metropolized PMGA (Algorithm 1 in Appendix I).\n",
    "\n",
    "def pmga_mh(y, h, K, N, X):\n",
    "    \"\"\"Metropolized Particle Marginal Gradient Ascent Algorithm.\n",
    "    Returns posterior variance estimates.\n",
    "    \"\"\"\n",
    "    D = y.size  # Extract dimension of latent variables.\n",
    "    th = np.array([theta_opt(X)])  # Compute initial parameter estimate.\n",
    "\n",
    "    # Initialize vectors containing the first two moments of the current\n",
    "    # particle cloud (required to compute the variance estimates):\n",
    "    mu1 = np.reshape(X.mean(1), (D, 1))  # First moment.\n",
    "    mu2 = np.reshape((X ** 2).mean(1), (D, 1)) # Second moment.\n",
    "\n",
    "    for k in range(K):\n",
    "        # Propose a new particle cloud:\n",
    "        Z = (X + h*grad_x(y, theta_opt(X), X)\n",
    "             + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        \n",
    "        # Accept-reject step:\n",
    "        if np.random.uniform(0, 1, 1) < accept(X, Z, y, h, N):\n",
    "            X = Z\n",
    "        th = np.append(th, theta_opt(X)) # Update parameter estimate.\n",
    "\n",
    "        # Store moments:\n",
    "        mu1 = np.append(mu1, np.reshape(X.mean(1), (D, 1)), axis=1)\n",
    "        mu2 = np.append(mu2, np.reshape((X ** 2).mean(1), (D, 1)), axis=1)\n",
    "\n",
    "    # Compute time-averaged variance estimates as a function of k:\n",
    "    var = (np.cumsum(mu2, axis=1)/np.arange(1, K + 2) \n",
    "           - (np.cumsum(mu1, axis=1)/np.arange(1, K + 2))**2)\n",
    "    return var  # Returns the coordinate-wise variances as a function of k.\n",
    "\n",
    "\n",
    "def accept(X, Z, y, h, N):\n",
    "    \"\"\"Computes acceptance probability for proposed moved X -> Z.\"\"\"\n",
    "    thx = theta_opt(X)\n",
    "    thz = theta_opt(Z)\n",
    "    s = 1\n",
    "    for n in range(N):\n",
    "        x = X[:, [n]]\n",
    "        z = Z[:, [n]]\n",
    "        tempx = ((y - x).T@(y - x)/2 + (x - thx).T@(x - thx)/2\n",
    "                 + (z - x - h*(y + thx - 2*x)).T\n",
    "                 @ (z - x - h*(y + thx - 2*x))/(4*h))\n",
    "        tempz = ((y - z).T@(y - z)/2 + (z - thz).T@(z - thz)/2\n",
    "                 + (x - z - h*(y + thz - 2*z)).T\n",
    "                 @ (x - z - h*(y + thz - 2*z))/(4*h))\n",
    "        s = s*np.exp(tempx - tempz)\n",
    "    return np.minimum(1, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0B0AvneG7U8"
   },
   "source": [
    "Regardless of the step-size h that we use, only bias stemming from the finite particle number N remains. However, for large particle numbers, we are forced to reduce the step-size to stop the acceptance probability from degenerating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "yvxMydgxF-Qq",
    "outputId": "557119c1-df59-4b25-ee79-753af8cb66a6"
   },
   "outputs": [],
   "source": [
    "K = 10000  # Number of steps.\n",
    "\n",
    "# Compute estimates:\n",
    "var1 = pmga_mh(y, 1/2, K, 1, np.zeros((D, 1)))  # h = 1/2 and N = 1.\n",
    "var2 = pmga_mh(y, 1/2, K, 2, np.zeros((D, 2)))  # h = 1/2 and N = 2.\n",
    "var4 = pmga_mh(y, 1/2, K, 4, np.zeros((D, 4)))  # h = 1/2 and N = 4.\n",
    "var50 = pmga_mh(y, 1/4, K, 50, np.zeros((D, 50)))  # h = 1/4 and N = 50.\n",
    "\n",
    "# Plot estimates:\n",
    "plt.plot(var1[0, :], label='N = 1')\n",
    "plt.plot(var2[0, :], label='N = 2')\n",
    "plt.plot(var4[0, :], label='N = 4')\n",
    "plt.plot(var50[0, :], label='N = 50')\n",
    "plt.plot(0.5*np.ones(K), label='Optimal variance')\n",
    "plt.legend(loc='lower right', ncol=2)\n",
    "plt.xlim([-K/100, K])\n",
    "plt.ylim([0, 1.2*var1[0, -1]])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNIs37wY22SNF0hlqNgmgn",
   "collapsed_sections": [
    "gcsX2yOfakVA",
    "PXeTOIZy-tZA",
    "ElXW-4d9B9nc"
   ],
   "include_colab_link": true,
   "name": "Toy Hierarchical Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
