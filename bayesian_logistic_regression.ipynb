{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ParticleEM/ParEM_anonymous/blob/main/bayesian_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5E8v2cZ6Zkf"
   },
   "source": [
    "**Description:** This notebook demonstrates the application of PGA, PQN, PMGA, and SOUL to the Bayesian logistic regression example in Section 4.1 of 'Scalable particle-based alternatives to EM' and reproduces Figure 3 and Table 2 therein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XotQU1Ootpi6"
   },
   "source": [
    "# Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErKddtPzM3_H"
   },
   "source": [
    "First, we load the modules that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRkQ4RDsIAGf"
   },
   "outputs": [],
   "source": [
    "#@title Load modules.\n",
    "\n",
    "# Install the wget package on Colab (if running the notebook locally,\n",
    "# comment the following line out).\n",
    "!pip install wget\n",
    "\n",
    "# OS and wget to load dataset.\n",
    "import os\n",
    "import wget\n",
    "\n",
    "# Numpy for computations.\n",
    "import numpy as np\n",
    "\n",
    "# Pyplot for plots.\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6VBGWiUNBLA"
   },
   "source": [
    "Next, we load and curate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mENE7XXBNLf0",
    "outputId": "508d7dbc-3134-4371-ff8b-4e166f9e9295"
   },
   "outputs": [],
   "source": [
    "#@title Load and normalize the Wisconsin Breast Cancer dataset.\n",
    "\n",
    "# Fetch dataset from repository:\n",
    "wget.download('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data')\n",
    "\n",
    "# Load dataset:\n",
    "dataset = np.loadtxt('breast-cancer-wisconsin.data', dtype=str, delimiter=',')\n",
    "\n",
    "# Delete local copy of dataset to avoid duplicates with multiple notebook runs:\n",
    "os.remove('breast-cancer-wisconsin.data')\n",
    "\n",
    "# Remove datapoints with missing attributes and change dtype to float:\n",
    "dataset = dataset[~(dataset == '?').any(axis=1), :].astype(float)\n",
    "\n",
    "# Extract features and labels, and normalize features:\n",
    "features = np.array(dataset[:, 1:10] - dataset[:, 1:10].mean(0))\n",
    "features = features/features.std(0)\n",
    "labels = np.array([(dataset[:, 10]-2)/2]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rf35vq3IC7T"
   },
   "source": [
    "We then implement the algorithms. They take the following inputs:\n",
    "\n",
    "*   l : training set labels,\n",
    "*   f : training set D-dimensional feature vectors,\n",
    "*   h : step-size,\n",
    "*   K : number of steps,\n",
    "*   N : number of particles,\n",
    "*   th : 1-dimensional vector with parameter guess,\n",
    "*   X : D x N matrix storing the initial particle cloud.\n",
    "\n",
    "They return the following outputs:\n",
    "\n",
    "*   th : K-dimensional vector of parameter estimates,\n",
    "*   X : D x (KN) matrix storing the particle clouds (the kth cloud is X[:, (k-1)$^\\ast$N : k$^\\ast$N])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LtqjvuN98ZmU"
   },
   "outputs": [],
   "source": [
    "#@title Implement algorithms.\n",
    "\n",
    "\n",
    "# Algorithms.\n",
    "\n",
    "def pga(l, f, h, K, N, th, X):\n",
    "    D = f[0, :].size  # Extract latent variable dimension.\n",
    "    for k in range(K):\n",
    "        Xk = X[:, -N:]  # Extract current particle cloud.\n",
    "        #Update particle cloud:\n",
    "        Xkp1 = (Xk + h*grad_x(th[k], Xk, l, f)\n",
    "                   + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        X = np.append(X, Xkp1, axis=1) # Store updated cloud.\n",
    "        th = np.append(th, th[k] + h*ave_grad_th(th[k], Xk))  # Update theta.\n",
    "    return th, X\n",
    "\n",
    "\n",
    "def pqn(l, f, h, K, N, th, X):\n",
    "    D = f[0, :].size  # Extract latent variable dimension.\n",
    "    for k in range(K):\n",
    "        Xk = X[:, -N:]  # Extract current particle cloud.\n",
    "        # Update particle cloud:\n",
    "        Xkp1 = (Xk + h*grad_x(th[k], Xk, l, f)\n",
    "                   + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        X = np.append(X, Xkp1, axis=1)  # Store updated cloud.\n",
    "        # Update theta:\n",
    "        th = np.append(th, th[k] \n",
    "                           + h*(ave_neg_hess_th(D)**-1)*ave_grad_th(th[k], Xk))  \n",
    "    return th, X\n",
    "\n",
    "\n",
    "def pmga(l, f, h, K, N, th, X):\n",
    "    D = f[0, :].size  # Extract latent variable dimension.\n",
    "    for k in range(K):\n",
    "        Xk = X[:, -N:]  # Extract current particle cloud.\n",
    "        # Update particle cloud:\n",
    "        Xkp1 = (Xk + h*grad_x(th[k], Xk, l, f)\n",
    "                   + np.sqrt(2*h)*np.random.normal(0, 1, (D, N)))\n",
    "        X = np.append(X, Xkp1, axis=1)  # Store updated cloud.\n",
    "        th = np.append(th, theta_opt(Xkp1))  # Update theta.\n",
    "    return th, X\n",
    "\n",
    "\n",
    "def soul(l, f, h, K, N, th, X):\n",
    "    D = f[0, :].size  # Extract latent variable dimension.\n",
    "    for k in range(K):\n",
    "        # Run ULA chain:\n",
    "        for n in range(N):\n",
    "            Xkn = X[:, -1].reshape(D, 1)  # Extract current particle position.\n",
    "            # Take a step:\n",
    "            Xknp1 = (Xkn + h*grad_x(th[k], Xkn, l, f)\n",
    "                          + np.sqrt(2*h)*np.random.normal(0, 1, (D, 1)))\n",
    "            X = np.append(X, Xknp1, axis=1)  # Store new particle position.\n",
    "        th = np.append(th, th[k] + h*ave_grad_th(th[k], X[:, -N:]))  # Update theta.\n",
    "    return th, X\n",
    "\n",
    "\n",
    "# Auxiliary functions.\n",
    "\n",
    "def ave_grad_th(th, x):\n",
    "    \"\"\"Returns theta-gradient of log density averaged over particle cloud.\"\"\"\n",
    "    return ((x-th).sum(0)).mean()/5\n",
    "\n",
    "\n",
    "def ave_neg_hess_th(D):\n",
    "    \"\"\"Returns negative-theta-Hessian of log density averaged over particles.\n",
    "    \"\"\"\n",
    "    return D/5\n",
    "\n",
    "\n",
    "def grad_x(th, x, l, f):\n",
    "    \"\"\"Returns x-gradient of log density vectorized over particles.\"\"\"\n",
    "    s = 1/(1+np.exp(- np.matmul(f, x)))\n",
    "    return np.matmul((l-s).transpose(), f).transpose() - (x-th)/5\n",
    "\n",
    "\n",
    "def theta_opt(X):\n",
    "    return X.mean()  # Return optimal parameter for particle cloud X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFb772_telnV"
   },
   "source": [
    "## Figure 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKT-6l_6urur"
   },
   "source": [
    "We run the algorithms using an 80/20 training/test split of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WG-_AlQR_S_"
   },
   "outputs": [],
   "source": [
    "# Split data into 80/20 training and testing sets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "ftrain, ftest, ltrain, ltest = train_test_split(features, labels, test_size=0.2, \n",
    "                                                random_state=0)\n",
    "\n",
    "# Set approximation parameters:\n",
    "h = 1e-2 # Step-size. \n",
    "K = 400  # Number of steps.\n",
    "N = 100  # Number of particles.\n",
    "D = features[0, :].size  # Dimension of latent space.\n",
    "\n",
    "# Initialize parameter estimates and particle cloud, all at zero:\n",
    "th0 = np.array([[0]])  # Parameter estimate.\n",
    "X0 = np.zeros((D, N))  # Particle cloud.\n",
    "\n",
    "# Run algorithms:\n",
    "th_pga, X_pga = pga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pqn, X_pqn = pqn(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pmga, X_pmga = pmga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_soul, X_soul = soul(ltrain, ftrain, h, K, N, th0, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6woA0pl4fEaB"
   },
   "source": [
    "And plot the parameter estimates as a function of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "R1XfFt7HfPDQ",
    "outputId": "c808fe40-19b6-4f07-c538-5beb4573e8e9"
   },
   "outputs": [],
   "source": [
    "plt.plot(th_pga, label='PGA') \n",
    "plt.plot(th_pqn, label='PQN')\n",
    "plt.plot(th_pmga, label='PMGA')\n",
    "plt.plot(th_soul, label='SOUL')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgyOR3kTgIBG"
   },
   "source": [
    "The estimates produced by all four algorithms converge to the same limit after similar transients. To observe more significant differences between the algorithms' outputs, we examine the posterior approximations they return..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYpeoZxVgozM"
   },
   "source": [
    "## Figure 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fancGkeOg5mU"
   },
   "source": [
    "The posterior approximations produced by SOUL are more peaked than those produced by PGA, PQN, and PMGA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "id": "PnduMMhdg2iN",
    "outputId": "7735e198-8312-468b-8baa-94b55ca27d73"
   },
   "outputs": [],
   "source": [
    "# Extract final particle clouds X^{1:N}_K:\n",
    "q_pga = X_pga[:, -N:]\n",
    "q_pqn = X_pqn[:, -N:]\n",
    "q_pmga = X_pmga[:, -N:]\n",
    "q_soul = X_soul[:, -N:]\n",
    "\n",
    "# Generate and plot KDEs:\n",
    "from scipy import stats  # stats to generate KDEs.\n",
    "fig = plt.figure(figsize=(27,3), dpi= 100)\n",
    "for i in range(D):\n",
    "    # Generate KDEs for ith entry of the final particle cloud X^{1:N}_K:\n",
    "    kde_min = np.min([q_pga[i, :], q_pqn[i, :], q_pmga[i, :], q_soul[i, :]])\n",
    "    kde_max = np.max([q_pga[i, :], q_pqn[i, :], q_pmga[i, :], q_soul[i, :]])\n",
    "    xaxis = np.linspace(kde_min, kde_max, num=100)\n",
    "\n",
    "    kde_pga = stats.gaussian_kde(q_pga[i, :])(xaxis)\n",
    "    kde_pqn = stats.gaussian_kde(q_pqn[i, :])(xaxis)\n",
    "    kde_pmga = stats.gaussian_kde(q_pmga[i, :])(xaxis)\n",
    "    kde_soul = stats.gaussian_kde(q_soul[i, :])(xaxis)\n",
    "\n",
    "    # Plot KDEs:\n",
    "    plt.subplot(1, D, i+1)\n",
    "    plt.plot(xaxis, kde_pga, label='PGA')\n",
    "    plt.plot(xaxis, kde_pqn, label='PQN')\n",
    "    plt.plot(xaxis, kde_pmga, label='PMGA')\n",
    "    plt.plot(xaxis, kde_soul, label='SOUL')\n",
    "    plt.title('Feature ' + str(i+1))\n",
    "    plt.ylim([0, 1.02*np.max([kde_pga, kde_pqn, kde_pmga, kde_soul])])\n",
    "    plt.xlim([kde_min, kde_max])\n",
    "    \n",
    "    \n",
    "handles, figlabels = plt.gca().get_legend_handles_labels()\n",
    "fig.legend(handles, figlabels, ncol=4, bbox_to_anchor=(0.36,1.3), \n",
    "           loc=\"upper center\",fontsize=20)\n",
    "plt.subplots_adjust(hspace=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m6qFUSb-tQB"
   },
   "source": [
    "## Figure 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fErNS-Ie_cLa"
   },
   "source": [
    "To spot clear differences between behaviour of the SOUL parameter estimates and that of the PGA, PQN, and PMGA parameter estimates, we initialize the estimates far from the optimal parameter and we initialize the particle cloud far from the corresponding posterior mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWO_MzWQBOZ4"
   },
   "outputs": [],
   "source": [
    "th0 = np.array([[10]])\n",
    "X0 = 10*np.ones((D, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxqGPtY_D6j8"
   },
   "source": [
    "With this initialization, the PGA, PQN, and PMGA estimates exhibit slower transients than the SOUL estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "8_fk5SeOEJED",
    "outputId": "5d053af9-823f-41e1-8fe1-e71c3ac60de2"
   },
   "outputs": [],
   "source": [
    "# Re-run algorithms:\n",
    "th_pga, X_pga = pga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pqn, X_pqn = pqn(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pmga, X_pmga = pmga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_soul, X_soul = soul(ltrain, ftrain, h, K, N, th0, X0)\n",
    "\n",
    "# Plot parameter estimates:\n",
    "plt.plot(th_pga, label='PGA') \n",
    "plt.plot(th_pqn, label='PQN')\n",
    "plt.plot(th_pmga, label='PMGA')\n",
    "plt.plot(th_soul, label='SOUL')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKqR7vxNF-13"
   },
   "source": [
    "However, we can cheaply burn off these long transients using a single-particle run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1OsaPtLGQ6n"
   },
   "outputs": [],
   "source": [
    "# Warm-start using a single-particle run:\n",
    "th_burn, X_burn = pmga(ltrain, ftrain, h, K, 1, th0, X0)\n",
    "th0 = th_burn[-1].reshape((1, 1))\n",
    "X0 = X_burn[:, -1].reshape((D, 1))*np.ones((1, N))  \n",
    "\n",
    "# Run the warm-started algorithms:\n",
    "th_pga, X_pga = pga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pqn, X_pqn = pqn(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_pmga, X_pmga = pmga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "th_soul, X_soul = soul(ltrain, ftrain, h, K, N, th0, X0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOXSESvuIkQ9"
   },
   "source": [
    "The estimates produced by all four warm-started algorithms are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "OPOHBDsWIjrP",
    "outputId": "ceb0d76d-24c7-4e53-ef7a-b82f631c131e"
   },
   "outputs": [],
   "source": [
    "plt.plot(th_pga, label='PGA') \n",
    "plt.plot(th_pqn, label='PQN')\n",
    "plt.plot(th_pmga, label='PMGA')\n",
    "plt.plot(th_soul, label='SOUL')\n",
    "plt.xlim([-K/100, K])\n",
    "plt.ylim([0.75, 1.25])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T62bmxVZ2V6q"
   },
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_STypNJCd0P"
   },
   "source": [
    "We benchmark performance of the algorithms by running them several times. First, we examine their predictive performance, which we evaluate using two metrics: the test error and log pointwise predictive density (see Appendix F.2 for definitions). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lc_eAPooDKdO"
   },
   "outputs": [],
   "source": [
    "#@title Performance metrics.\n",
    "\n",
    "def predict(f, X):\n",
    "    \"\"\"Returns label maximizing the approximate posterior predictive \n",
    "    distribution defined by the cloud X, vectorized over feature vectors f.\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(- np.matmul(f, X))).mean(1)\n",
    "    out = np.zeros((f[:, 0].size, 1))\n",
    "    out[s >= 1/2] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def test_error(f, l, X):\n",
    "    \"\"\"Returns fraction of misclassified test points.\"\"\"\n",
    "    return (np.abs(l - predict(f, X))).mean()\n",
    "\n",
    "\n",
    "def lppd(f, l, X):\n",
    "    \"\"\"Returns log pointwise predictive density.\"\"\"\n",
    "    s = 1/(1+np.exp(- np.matmul(f, X)))\n",
    "    return np.log((((s ** l) * ((1-s) ** (1-l))).mean(1))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHf8uU4FUMhr"
   },
   "source": [
    "## Predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evxgxACjMhfc"
   },
   "source": [
    "We compute the metrics for M=100 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nO6q_z88Mlyh",
    "outputId": "829e9e58-4667-4db5-f69c-dfeb69f5c9c1"
   },
   "outputs": [],
   "source": [
    "M = 100  # Number of runs.\n",
    "N = 100 # Number of particles.\n",
    "kb = int(K/2)  # Number of burn-in steps.\n",
    "\n",
    "# Set initial conditions to zero:\n",
    "th0 = np.array([[0]])  # Parameter estimates.\n",
    "X0 = np.zeros((D, N))  # Particle cloud.\n",
    "\n",
    "# Initialize arrays storing the LPPDs and test errors:\n",
    "lppd_pga = np.zeros((M))\n",
    "lppd_pqn = np.zeros((M))\n",
    "lppd_pmga = np.zeros((M))\n",
    "lppd_soul = np.zeros((M))\n",
    "\n",
    "error_pga = np.zeros((M))\n",
    "error_pqn = np.zeros((M))\n",
    "error_pmga = np.zeros((M))\n",
    "error_soul = np.zeros((M))\n",
    "\n",
    "for m in range(M):\n",
    "    # Randomize the 80/20 testing/training split of the data:\n",
    "    ftrain, ftest, ltrain, ltest = train_test_split(features, labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Run the algorithms:\n",
    "    _, X_pga = pga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    _, X_pqn = pqn(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    _, X_pmga = pmga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    _, X_soul = soul(ltrain, ftrain, h, K, N, th0, X0)\n",
    "\n",
    "    #Compute LPPDs:\n",
    "    lppd_pga[m] = lppd(ftest, ltest, X_pga[:, N*kb:])\n",
    "    lppd_pqn[m] = lppd(ftest, ltest, X_pqn[:, N*kb:])\n",
    "    lppd_pmga[m] = lppd(ftest, ltest, X_pmga[:, N*kb:])\n",
    "    lppd_soul[m] = lppd(ftest, ltest, X_soul[:, N*kb:])\n",
    "\n",
    "    # Compute test errors:\n",
    "    error_pga[m] = test_error(ftest, ltest, X_pga[:, N*kb:])\n",
    "    error_pqn[m] = test_error(ftest, ltest, X_pqn[:, N*kb:])\n",
    "    error_pmga[m] = test_error(ftest, ltest, X_pmga[:, N*kb:])\n",
    "    error_soul[m] = test_error(ftest, ltest, X_soul[:, N*kb:])\n",
    "\n",
    "#Print statistics (mean +- standard deviation):\n",
    "print('LPPDs:')\n",
    "print('PGA: '+str(lppd_pga.mean())+' \\u00B1 '+str(lppd_pga.std())+'.')\n",
    "print('PQN: '+str(lppd_pqn.mean())+' \\u00B1 '+str(lppd_pqn.std())+'.')\n",
    "print('PMGA: '+str(lppd_pmga.mean())+' \\u00B1 '+str(lppd_pmga.std())+'.')\n",
    "print('SOUL: '+str(lppd_soul.mean())+' \\u00B1 '+str(lppd_soul.std())+'.')\n",
    "\n",
    "print('Test errors:')\n",
    "print('PGA: '+str(error_pga.mean())+' \\u00B1 '+str(error_pga.std())+'.')\n",
    "print('PQN: '+str(error_pqn.mean())+' \\u00B1 '+str(error_pqn.std())+'.')\n",
    "print('PMGA: '+str(error_pmga.mean())+' \\u00B1 '+str(error_pmga.std())+'.')\n",
    "print('SOUL: '+str(error_soul.mean())+' \\u00B1 '+str(error_soul.std())+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J-lrd8xOsXM"
   },
   "source": [
    "## Stationary variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3javdBWHq6dv"
   },
   "source": [
    "Next, we compute the stationary variances of the parameter estimates. To ensure that all algorithms reach stationarity, we warm-start them using a single particle run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6adyY1ROu2T",
    "outputId": "c23bc806-54f0-4a1c-e988-4c375fac7105"
   },
   "outputs": [],
   "source": [
    "# Warm-start using a single-particle run:\n",
    "th_burn, X_burn = pmga(ltrain, ftrain, h, K, 1, th0, X0)\n",
    "th0 = th_burn[-1].reshape((1, 1))\n",
    "X0 = X_burn[:, -1].reshape((D, 1))*np.ones((1, N))  \n",
    "\n",
    "# Initialize arrays storing the stationary variances:\n",
    "var_pga = np.zeros((M))\n",
    "var_pqn = np.zeros((M))\n",
    "var_pmga = np.zeros((M))\n",
    "var_soul = np.zeros((M))\n",
    "\n",
    "for m in range(M):\n",
    "    # Randomize the 80/20 testing/training split of the data:\n",
    "    ftrain, ftest, ltrain, ltest = train_test_split(features, labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=0)\n",
    "\n",
    "    # Run the algorithms:\n",
    "    th_pga, _ = pga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    th_pqn, _ = pqn(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    th_pmga, _ = pmga(ltrain, ftrain, h, K, N, th0, X0)\n",
    "    th_soul, _ = soul(ltrain, ftrain, h, K, N, th0, X0)\n",
    "\n",
    "    # Compute variances:\n",
    "    var_pga[m] = th_pga[kb:].var()\n",
    "    var_pqn[m] = th_pqn[kb:].var()\n",
    "    var_pmga[m] = th_pmga[kb:].var()\n",
    "    var_soul[m] = th_soul[kb:].var()\n",
    "\n",
    "\n",
    "# Print statistics (mean +- standard deviation):\n",
    "print('Stationary variances:')\n",
    "print('PGA: '+str(var_pga.mean())+' \\u00B1 '+str(var_pga.std())+'.')\n",
    "print('PQN: '+str(var_pqn.mean())+' \\u00B1 '+str(var_pqn.std())+'.')\n",
    "print('PMGA: '+str(var_pmga.mean())+' \\u00B1 '+str(var_pmga.std())+'.')\n",
    "print('SOUL: '+str(var_soul.mean())+' \\u00B1 '+str(var_soul.std())+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hb2K-quMUSHt"
   },
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9qax-8sEifB"
   },
   "source": [
    "Lastly, we obtain runtime statistics using Python's timeit module (note that Colab can be set to run on a CPU, GPU, or TPU by selecting Runtime --> Change runtime type --> Hardware accelerator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5wqPOeGYEYd",
    "outputId": "20dda85e-18e0-4633-9a4c-994e55806ebc"
   },
   "outputs": [],
   "source": [
    "from timeit import repeat\n",
    "\n",
    "# Time the algorithms:\n",
    "t_pga = np.array(repeat(stmt='pga(ltrain, ftrain, h, K, N, th0, X0)',\n",
    "                        number=1, repeat=M, globals=globals()))\n",
    "t_pqn = np.array(repeat(stmt='pqn(ltrain, ftrain, h, K, N, th0, X0)',\n",
    "                        number=1, repeat=M, globals=globals()))\n",
    "t_pmga = np.array(repeat(stmt='pmga(ltrain, ftrain, h, K, N, th0, X0)',\n",
    "                         number=1, repeat=M, globals=globals()))\n",
    "t_soul = np.array(repeat(stmt='soul(ltrain, ftrain, h, K, N, th0, X0)',\n",
    "                         number=1, repeat=M, globals=globals()))\n",
    "\n",
    "# Print statistics (mean +- standard deviation):\n",
    "print('Runtimes:')\n",
    "print('PGA: '+str(t_pga.mean())+' \\u00B1 '+str(t_pga.std())+'.')\n",
    "print('PQN: '+str(t_pqn.mean())+' \\u00B1 '+str(t_pqn.std())+'.')\n",
    "print('PMGA: '+str(t_pmga.mean())+' \\u00B1 '+str(t_pmga.std())+'.')\n",
    "print('SOUL: '+str(t_soul.mean())+' \\u00B1 '+str(t_soul.std())+'.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP2Omd2UCGOvaulsWFR5ssa",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Bayesian Logistic Regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
